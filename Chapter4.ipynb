{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter04.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('anaconda3': virtualenv)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"interpreter":{"hash":"be4b63defce17d5a080b8f465a6563e41eb7f57f474d17922a3f2b4d2ad27abe"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8R27gfyksk4d"},"source":["# 4章\n","- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"]},{"cell_type":"code","metadata":{"id":"kvqSUAEtU_VJ"},"source":["# ライブラリをインポートする。\n","!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.5.0\n","  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 7.7 MB/s \n","\u001b[?25hCollecting fugashi==1.1.0\n","  Downloading fugashi-1.1.0-cp38-cp38-macosx_10_14_x86_64.whl (282 kB)\n","\u001b[K     |████████████████████████████████| 282 kB 54.2 MB/s \n","\u001b[?25hCollecting ipadic==1.0.0\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[K     |████████████████████████████████| 13.4 MB 30.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (1.19.2)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (2020.10.15)\n","Requirement already satisfied: requests in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (2.24.0)\n","Requirement already satisfied: tqdm>=4.27 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (4.50.2)\n","Requirement already satisfied: filelock in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-macosx_10_11_x86_64.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 61.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (20.4)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 37.5 MB/s \n","\u001b[?25hRequirement already satisfied: six in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers==4.5.0) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers==4.5.0) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (2020.6.20)\n","Requirement already satisfied: click in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n","Requirement already satisfied: joblib in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.5.0) (0.17.0)\n","Building wheels for collected packages: ipadic\n","  Building wheel for ipadic (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556722 sha256=39e48e6803b5b9241ee4c9b3c7668b8e81de09220b48126019476a1e5026229e\n","  Stored in directory: /Users/harukikondo/Library/Caches/pip/wheels/45/b7/f5/a21e68db846eedcd00d69e37d60bab3f68eb20b1d99cdff652\n","Successfully built ipadic\n","Installing collected packages: tokenizers, sacremoses, transformers, ipadic, fugashi\n","Successfully installed fugashi-1.1.0 ipadic-1.0.0 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.5.0\n","\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n","You should consider upgrading via the '/Users/harukikondo/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"DWT32lOgHLrU"},"source":["import torch\n","from transformers import BertJapaneseTokenizer, BertModel"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcFsCeMBZVDR"},"source":["# モデルには、東北大学の研究チームの事前学習モデルを指定する。\n","model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n","# トークナイザを用意する。\n","tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: 100%|██████████| 258k/258k [00:00<00:00, 1.06MB/s]\n","Downloading: 100%|██████████| 110/110 [00:00<00:00, 53.1kB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"gPy-VLclxU_u"},"source":["# 4-4\n","tokenizer.tokenize('明日は自然言語処理の勉強をしよう。')"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', 'う', '。']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"fekTFJmD0-TQ"},"source":["# 「#」は、単語の一番最初以外のトークンに付与される。\n","tokenizer.tokenize('明日はマシンラーニングの勉強をしよう。')"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['明日', 'は', 'マシン', '##ラー', '##ニング', 'の', '勉強', 'を', 'しよ', 'う', '。']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"g35K_yPf4YZL"},"source":["# [UNK]は、未知の単語を表すトークン\n","tokenizer.tokenize('機械学習を中国語にすると机器学习だ。')"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['機械', '学習', 'を', '中国', '語', 'に', 'する', 'と', '机', '器', '学', '[UNK]', 'だ', '。']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['PC',\n"," '##I',\n"," 'DS',\n"," '##S',\n"," 'の',\n"," '要件',\n"," 'の',\n"," '一つ',\n"," 'に',\n"," 'は',\n"," '、',\n"," 'ファイア',\n"," '##ウォール',\n"," 'を',\n"," '設置',\n"," 'する',\n"," 'こと',\n"," 'が',\n"," '含ま',\n"," 'れ',\n"," 'て',\n"," 'いる',\n"," '。']"]},"metadata":{},"execution_count":10}],"source":["# ファイアウォールを含んだ文章をトークン化\n","tokenizer.tokenize('PCI DSSの要件の一つには、ファイアウォールを設置することが含まれている。')"]},{"cell_type":"code","metadata":{"id":"pWVOdX-Ci7zx"},"source":["# トークンをさらにID化(符号化する)\n","input_ids = tokenizer.encode('明日は自然言語処理の勉強をしよう。')\n","print(input_ids)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 11475, 9, 1757, 1882, 2762, 5, 8192, 11, 2132, 205, 8, 3]\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 3794, 28645, 7126, 28583, 5, 8909, 5, 993, 7, 9, 6, 19764, 25693, 11, 783, 34, 45, 14, 1610, 20, 16, 33, 8, 3]\n"]}],"source":["# ファイアウォールを含んだ文章をID列化\n","input_ids2 =  tokenizer.encode('PCI DSSの要件の一つには、ファイアウォールを設置することが含まれている。')\n","print(input_ids2)"]},{"cell_type":"code","metadata":{"id":"QNAPAMjCjmH-"},"source":["# ID列をトークン列に変換する。\n","tokenizer.convert_ids_to_tokens(input_ids)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', '明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', 'う', '。', '[SEP]']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"FCOXUJCsxj_F"},"source":["# 複数の文章を取り扱うために、トークン列の長さを統一する処理\n","text = '明日の天気は晴れだ。'\n","# ID列化\n","encoding = tokenizer(\n","    text, max_length=12, padding='max_length', truncation=True\n",")\n","print('# encoding:')\n","print(encoding)\n","# トークン化\n","tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n","print('# tokens:')\n","print(tokens)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["# encoding:\n{'input_ids': [2, 11475, 5, 11385, 9, 16577, 75, 8, 3, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}\n# tokens:\n['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"]}]},{"cell_type":"code","metadata":{"id":"Y_KCd86ozaYH"},"source":["# max_lengthを6に変更する。\n","encoding = tokenizer(\n","    text, max_length=6, padding='max_length', truncation=True\n",")\n","tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n","print(tokens)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', '明日', 'の', '天気', 'は', '[SEP]']\n"]}]},{"cell_type":"code","metadata":{"id":"Lty7haD0kG-U"},"source":["# 複数の文章をID列化する。\n","text_list = ['明日の天気は晴れだ。','パソコンが急に動かなくなった。']\n","tokenizer(\n","    text_list, max_length=10, padding='max_length', truncation=True\n",")"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 11475, 5, 11385, 9, 16577, 75, 8, 3, 0], [2, 6311, 14, 1132, 7, 16084, 332, 58, 10, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"Sv2IQ2uD2B1i"},"source":["# 文字列が最大のものに合わせる。\n","tokenizer(text_list, padding='longest')"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 11475, 5, 11385, 9, 16577, 75, 8, 3, 0, 0], [2, 6311, 14, 1132, 7, 16084, 332, 58, 10, 8, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"e9f1dK4IDzt_"},"source":["# ID列化 + 数値配列をテンソル化する。\n","tokenizer(\n","    text_list,\n","    max_length=10,\n","    padding='max_length',\n","    truncation=True,\n","    return_tensors='pt'\n",")"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    2, 11475,     5, 11385,     9, 16577,    75,     8,     3,     0],\n","        [    2,  6311,    14,  1132,     7, 16084,   332,    58,    10,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"6ddHZWk6wLjh"},"source":["# モデルのロード\n","model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n","bert = BertModel.from_pretrained(model_name)\n","# BERTをGPUに載せる\n","bert = bert.cuda() "],"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: 100%|██████████| 479/479 [00:00<00:00, 220kB/s]\n","Downloading: 100%|██████████| 445M/445M [00:06<00:00, 66.2MB/s]\n"]},{"output_type":"error","ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-e742af706c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# BERTをGPUに載せる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}]},{"cell_type":"code","metadata":{"id":"jUG0FwjdERPP"},"source":["# モデルの設定項目を出力する。\n","# max_position_embeddings 最大で入力できるトークン列の長さは512\n","# num_hidden_layers 隠れ層の数\n","# hidden_size BERTの出力の次元数\n","# モデルのパラメーター数 約1億1千万個\n","print(bert.config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4Pcz2VCCChM"},"source":["# 今回入力する文字列\n","text_list = [\n","    '明日は自然言語処理の勉強をしよう。',\n","    '明日はマシーンラーニングの勉強をしよう。'\n","]\n","\n","# 文章の符号化\n","encoding = tokenizer(\n","    text_list,\n","    max_length=32,\n","    padding='max_length',\n","    truncation=True,\n","    return_tensors='pt'\n",")\n","\n","# データをGPUに載せる\n","encoding = { k: v.cuda() for k, v in encoding.items() } \n","\n","# BERTでの処理\n","# それぞれの入力は2次元のtorch.Tensor\n","output = bert(**encoding) \n","# 最終層の出力\n","last_hidden_state = output.last_hidden_state "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwmt3sF3gU1L"},"source":["# bert(**encoding) と意味は同一\n","output = bert(\n","    input_ids=encoding['input_ids'], \n","    attention_mask=encoding['attention_mask'],\n","    token_type_ids=encoding['token_type_ids']\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6uDcHAPSFVlF"},"source":["# テンソルのサイズの出力する。\n","# バッチサイズ(まとめて処理する文章の数)、系列長、隠れ状態の次元数\n","print(last_hidden_state.size()) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-FPInM0KaPqh"},"source":["# BERTでの推論を行う時のコード\n","with torch.no_grad():\n","    output = bert(**encoding)\n","    last_hidden_state = output.last_hidden_state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rxAZqhrmTZOM"},"source":["# CPUにうつす。\n","last_hidden_state = last_hidden_state.cpu() \n","# numpy.ndarrayに変換\n","last_hidden_state = last_hidden_state.numpy() \n","# リストに変換\n","last_hidden_state = last_hidden_state.tolist() "],"execution_count":null,"outputs":[]}]}