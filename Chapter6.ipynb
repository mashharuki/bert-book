{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter06.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('anaconda3': virtualenv)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"interpreter":{"hash":"be4b63defce17d5a080b8f465a6563e41eb7f57f474d17922a3f2b4d2ad27abe"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DKcIdYD2sySs"},"source":["# 6章\n","- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"]},{"cell_type":"code","metadata":{"id":"BDX6Gi6xiCOY"},"source":["# chap6ディレクトリを作成して、移動する。\n","!mkdir chap6\n","%cd ./chap6"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/Users/harukikondo/git/bert-book/chap6\n"]}]},{"cell_type":"code","metadata":{"id":"0hJ-pXOwXBzH"},"source":["# 必要なライブラリをインポートする。\n","!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.7"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.5.0 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (4.5.0)\n","Requirement already satisfied: fugashi==1.1.0 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (1.1.0)\n","Requirement already satisfied: ipadic==1.0.0 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (1.0.0)\n","Collecting pytorch-lightning==1.2.7\n","  Downloading pytorch_lightning-1.2.7-py3-none-any.whl (830 kB)\n","\u001b[K     |████████████████████████████████| 830 kB 8.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (4.50.2)\n","Requirement already satisfied: requests in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (2.24.0)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (2020.10.15)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (1.19.2)\n","Requirement already satisfied: filelock in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (3.0.12)\n","Requirement already satisfied: sacremoses in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (0.0.45)\n","Requirement already satisfied: packaging in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.5.0) (20.4)\n","Collecting torchmetrics>=0.2.0\n","  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n","\u001b[K     |████████████████████████████████| 234 kB 85.9 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (1.7.1)\n","Collecting tensorboard>=2.2.0\n","  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 55.0 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>=0.8.1 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (0.8.3)\n","Requirement already satisfied: future>=0.17.1 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (0.18.2)\n","Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.2.7) (5.3.1)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp38-cp38-macosx_10_14_x86_64.whl (648 kB)\n","\u001b[K     |████████████████████████████████| 648 kB 92.3 MB/s \n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n","  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n","\u001b[K     |████████████████████████████████| 781 kB 68.0 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.35.1)\n","Collecting absl-py>=0.4\n","  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 54.7 MB/s \n","\u001b[?25hCollecting markdown>=2.6.8\n","  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 26.6 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (3.13.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.0.1)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.30.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.37.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (50.3.1.post20201107)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 60.0 MB/s \n","\u001b[?25hRequirement already satisfied: six in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (4.7.2)\n","Collecting requests-oauthlib>=0.7.0\n","  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.5.0) (2020.6.20)\n","Collecting oauthlib>=3.0.0\n","  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n","\u001b[K     |████████████████████████████████| 146 kB 49.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing_extensions in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.4->pytorch-lightning==1.2.7) (3.7.4.3)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.1.0-cp38-cp38-macosx_10_14_x86_64.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 11.4 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (20.3.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.6.3-cp38-cp38-macosx_10_14_x86_64.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 71.4 MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pyparsing>=2.0.2 in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers==4.5.0) (2.4.7)\n","Requirement already satisfied: joblib in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.5.0) (0.17.0)\n","Requirement already satisfied: click in /Users/harukikondo/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n","Installing collected packages: oauthlib, multidict, yarl, requests-oauthlib, async-timeout, tensorboard-plugin-wit, tensorboard-data-server, markdown, google-auth-oauthlib, aiohttp, absl-py, torchmetrics, tensorboard, pytorch-lightning\n","Successfully installed absl-py-0.13.0 aiohttp-3.7.4.post0 async-timeout-3.0.1 google-auth-oauthlib-0.4.4 markdown-3.3.4 multidict-5.1.0 oauthlib-3.1.1 pytorch-lightning-1.2.7 requests-oauthlib-1.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 torchmetrics-0.4.1 yarl-1.6.3\n","\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n","You should consider upgrading via the '/Users/harukikondo/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"V_BGiKTflI39"},"source":["# 必要なライブラリをインポーナイ座ー\n","import random\n","import glob\n","from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n","import pytorch_lightning as pl\n","\n","# 日本語の事前学習モデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzgAG-1VpLd7"},"source":["# トークナイザーを用意する。\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","# 文章分類用のモデルをロードする。\n","bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n","# GPUに配置する。\n","bert_sc = bert_sc.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6EbYOsCGzaC"},"source":["# 文章のデータセット\n","text_list = [\n","    \"この映画は面白かった。\",\n","    \"この映画の最後にはがっかりさせられた。\",\n","    \"この映画を見て幸せな気持ちになった。\"\n","]\n","# 判定のデータセット\n","label_list = [1,0,1]\n","\n","# データの符号化\n","encoding = tokenizer(text_list, padding = 'longest', return_tensors='pt')\n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","# テンソル化する。\n","labels = torch.tensor(label_list).cuda()\n","\n","# 推論\n","with torch.no_grad():\n","    # BERTモデルに入力する。\n","    output = bert_sc.forward(**encoding)\n","# 分類スコアを取得する。\n","scores = output.logits \n","# スコアが最も高いラベル\n","labels_predicted = scores.argmax(-1) \n","# 正解数\n","num_correct = (labels_predicted==labels).sum().item() \n","# 精度\n","accuracy = num_correct/labels.size(0) \n","# 結果を出力する。\n","print(\"# scores:\")\n","print(scores.size())\n","print(\"# predicted labels:\")\n","print(labels_predicted)\n","print(\"# accuracy:\")\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtKgd11pGyiE"},"source":["# 符号化\n","encoding = tokenizer(text_list, padding='longest', return_tensors='pt') \n","# 入力にラベルを加える。\n","encoding['labels'] = torch.tensor(label_list) \n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","# ロスの計算\n","output = bert_sc(**encoding)\n","# 損失の取得\n","loss = output.loss \n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r97ZbgVeZ-Hi"},"source":["# 6-7\n","#データのダウンロード\n","!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n","#ファイルの解凍\n","!tar -zxf ldcc-20140209.tar.gz "],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["zsh:1: command not found: wget\n","tar: Error opening archive: Failed to open 'ldcc-20140209.tar.gz'\n"]}]},{"cell_type":"code","metadata":{"id":"TMUJ3rscgG2z"},"source":["# 6-8\n","!cat ./text/it-life-hack/it-life-hack-6342280.txt # ファイルを表示"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["http://news.livedoor.com/article/detail/6342280/\n2012-03-06T13:00:00+0900\nUSB3.0対応で爆速データ転送！　9倍速のリーダー／ライター登場\nUSB3.0が登場してから今年で4年目となるがパソコン側でのUSB3.0ポート搭載が進んで来ても対応機器がなかなか充実していない現状がある。そんな中で新しく高速な読み取りが可能なメモリーカードリーダー／ライターが登場した。\n\nバッファローコクヨサプライがUSB3.0対応のカードリーダー／ライターを発表した。SDHC対応のSD系メディアやコンパクトフラッシュ、メモリースティック系メディア、xDピクチャーカードといったデジカメやスマホ、携帯ゲームといった機器で使われている各種メディアを従来よりも短時間でPCに取り込むことが可能になる。\n\n転送速度が5Gbps（理論値）とUSB2.0の480Mbpsと比べて爆速になったUSB3.0はPC側の対応が進んで来ていたが高速転送が生かせる周辺機器としては、外付けHDDや一部のUSBメモリーくらいしかなかった。これに多くのメディアが扱えるリーダー／ライターが加わることで手軽にUSB3.0の恩恵を受けることができるようになる。\n\n今回発表されたのは、USB3.0ケーブルとカードリーダー本体が分かれるタイプの「BSCR09U3」シリーズ（3,240円）、USB3.0コネクタをカードリーダー本体に内蔵している「BSCRD04U3」シリーズ（2,690円）だ。共にホワイトとブラックのカラーバリエーションが用意される（発売は3月下旬以降）。\n\n■リリースページ\n■バッファローコクヨサプライ\n\n\n\n\n■バッファローの記事をもっと見る\n・約283gでカバンに入る！小型キーボードの驚くべき機能\n・3種類のホットキーで使いやすい！AndroidとPCで使えるキーボードの魅力\n・ドラえもんもビックリの新アイテム！マウスとキーボードが合体\"OPAir\"\n・ありそうでなかった便利機能！ファイル仕分けする画期的なHDD\n\n\nサンディスク SanDisk microSDHC 32GB（microSD 32GB） 超高速クラス4  変換アダプター付 世界国内シェアNo.1 バルク品\nクチコミを見る\n"]}]},{"cell_type":"code","metadata":{"id":"49pchD2z6JhM"},"source":["# データローダーの作成\n","dataset_for_loader = [\n","    {'data':torch.tensor([0,1]), 'labels':torch.tensor(0)},\n","    {'data':torch.tensor([2,3]), 'labels':torch.tensor(1)},\n","    {'data':torch.tensor([4,5]), 'labels':torch.tensor(2)},\n","    {'data':torch.tensor([6,7]), 'labels':torch.tensor(3)},\n","]\n","loader = DataLoader(dataset_for_loader, batch_size=2)\n","\n","# データセットからミニバッチを取り出す\n","for idx, batch in enumerate(loader):\n","    print(f'# batch {idx}')\n","    print(batch)\n","    ## ファインチューニングではここでミニバッチ毎の処理を行う"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_1f6IbMVbaH"},"source":["# データセットからランダムにデータを取り出す。\n","loader = DataLoader(dataset_for_loader, batch_size=2, shuffle=True)\n","# データセットからミニバッチを取り出す\n","for idx, batch in enumerate(loader):\n","    print(f'# batch {idx}')\n","    print(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9YGEfZUAxea"},"source":["# カテゴリーのリスト\n","category_list = [\n","    'dokujo-tsushin',\n","    'it-life-hack',\n","    'kaden-channel',\n","    'livedoor-homme',\n","    'movie-enter',\n","    'peachy',\n","    'smax',\n","    'sports-watch',\n","    'topic-news'\n","]\n","# トークナイザのロード\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","# 各データの形式を整える\n","max_length = 128\n","# データローダーを格納するための配列\n","dataset_for_loader = []\n","for label, category in enumerate(tqdm(category_list)):\n","    for file in glob.glob(f'./text/{category}/{category}*'):\n","        lines = open(file).read().splitlines()\n","        # ファイルの4行目からを抜き出す。\n","        text = '\\n'.join(lines[3:]) \n","        # トークンを符号化する。\n","        encoding = tokenizer(\n","            text,\n","            max_length=max_length, \n","            padding='max_length',\n","            truncation=True\n","        )\n","        # ラベルを追加する。\n","        encoding['labels'] = label \n","        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","        # 整形済みのデータを追加する。\n","        dataset_for_loader.append(encoding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drP8IYLVBFh_"},"source":["# データセットを出力する。\n","print(dataset_for_loader[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHY9Os6NJlip"},"source":["# データセットの分割\n","# ランダムにシャッフル\n","random.shuffle(dataset_for_loader) \n","n = len(dataset_for_loader)\n","n_train = int(0.6*n)\n","n_val = int(0.2*n)\n","# 学習データ\n","dataset_train = dataset_for_loader[:n_train] \n","# 検証データ\n","dataset_val = dataset_for_loader[n_train:n_train+n_val] \n","# テストデータ\n","dataset_test = dataset_for_loader[n_train+n_val:] \n","\n","# データセットからデータローダを作成\n","# 学習データはshuffle=Trueにする。\n","dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True) \n","dataloader_val = DataLoader(dataset_val, batch_size=256)\n","dataloader_test = DataLoader(dataset_test, batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffaUyGcoVj8l"},"source":["# モデルの振る舞いを定義したクラス\n","class BertForSequenceClassification_pl(pl.LightningModule):\n","        \n","    # model_name: Transformersのモデルの名前\n","    # num_labels: ラベルの数\n","    # lr: 学習率\n","    def __init__(self, model_name, num_labels, lr):\n","\n","        super().__init__()\n","        \n","        # 引数のnum_labelsとlrを保存。\n","        # 例えば、self.hparams.lrでlrにアクセスできる。\n","        # チェックポイント作成時にも自動で保存される。\n","        self.save_hyperparameters() \n","        # BERTのロード\n","        self.bert_sc = BertForSequenceClassification.from_pretrained(model_name,num_labels=num_labels)\n","        \n","    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n","    # batch_idxはミニバッチの番号であるが今回は使わない。\n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        loss = output.loss\n","        # 損失を'train_loss'の名前でログをとる。\n","        self.log('train_loss', loss) \n","        return loss\n","        \n","    # 検証データのミニバッチが与えられた時に、\n","    # 検証データを評価する指標を計算する関数を書く。\n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        val_loss = output.loss\n","        # 損失を'val_loss'の名前でログをとる。\n","        self.log('val_loss', val_loss) \n","\n","    # テストデータのミニバッチが与えられた時に、\n","    # テストデータを評価する指標を計算する関数を書く。\n","    def test_step(self, batch, batch_idx):\n","        # バッチからラベルを取得\n","        labels = batch.pop('labels') \n","        # BERTに入力する。\n","        output = self.bert_sc(**batch)\n","        # もっとも高いスコアのものを取り出す。\n","        labels_predicted = output.logits.argmax(-1)\n","        num_correct = ( labels_predicted == labels ).sum().item()\n","        #精度\n","        accuracy = num_correct/labels.size(0) \n","        # 精度を'accuracy'の名前でログをとる。\n","        self.log('accuracy', accuracy) \n","\n","    # 学習に用いるオプティマイザを返す関数を書く。\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyR6de1TqfW9"},"source":["# ファインチューニング用の設定を行う。\n","# 学習時にモデルの重みを保存する条件を指定\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/',\n",")\n","\n","# 学習の方法を指定\n","# gpuを1つ資料する。\n","# エポック数 10\n","# モデルの重みを保存するタイミング\n","trainer = pl.Trainer(\n","    gpus=1, \n","    max_epochs=10,\n","    callbacks = [checkpoint]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgk48zEqIJKh"},"source":["# PyTorch Lightningモデルのロード\n","model = BertForSequenceClassification_pl(MODEL_NAME, num_labels=9, lr=1e-5)\n","# ファインチューニングを行う。\n","trainer.fit(model, dataloader_train, dataloader_val) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h68P7MG-JSh9"},"source":["# ベストモデルのファイル\n","best_model_path = checkpoint.best_model_path \n","print('ベストモデルのファイル: ', checkpoint.best_model_path)\n","print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-r9stqZqBdW"},"source":["# 学習結果を出力する。\n","%load_ext tensorboard\n","%tensorboard --logdir ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bx0L0Ehr1tM"},"source":["# テストデータを渡して検証する。\n","test = trainer.test(test_dataloaders=dataloader_test)\n","print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbJAUdrStSgI"},"source":["# ファインチューニングしたモデルを使用する。\n","# PyTorch Lightningモデルのロード\n","model = BertForSequenceClassification_pl.load_from_checkpoint(best_model_path) \n","\n","# Transformers対応のモデルを./model_transformesに保存\n","model.bert_sc.save_pretrained('./model_transformers') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcho1B0BtfV0"},"source":["# ファインチューニングしたモデルのデータを読み込んでモデルをロードする。\n","bert_sc = BertForSequenceClassification.from_pretrained('./model_transformers')"],"execution_count":null,"outputs":[]}]}